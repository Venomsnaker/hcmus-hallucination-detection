{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_curve, auc\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc"
   ],
   "id": "20182471916228d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!unzip",
   "id": "8bbea726d9b826e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_mean(input_list):\n",
    "    temp = [torch.mean(x, dim=0).squeeze(0) for x in input_list]\n",
    "    return torch.stack(temp).to(temp+[0].device)\n",
    "\n",
    "class DetectorModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        w_qa_embedding=0.2,\n",
    "        w_qa_gradient=0.2,\n",
    "        w_ia_embedding=0.2,\n",
    "        w_ia_gradient=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.w_qa_embedding = w_qa_embedding\n",
    "        self.w_qa_gradient = w_qa_gradient\n",
    "        self.w_ia_embedding = w_ia_embedding\n",
    "        self.w_ia_gradient = w_ia_gradient\n",
    "\n",
    "    def forward(self, qa_embedding, qa_gradient, ia_embedding, ia_gradient):\n",
    "        qa_embedding = get_mean(qa_embedding)\n",
    "        qa_gradient = get_mean(qa_gradient)\n",
    "        ia_embedding = get_mean(ia_embedding)\n",
    "        ia_gradient = get_mean(ia_gradient)\n",
    "\n",
    "        x = (\n",
    "            self.w_qa_embedding * qa_embedding\n",
    "            + self.w_qa_gradient * qa_gradient\n",
    "            + self.w_ia_embedding * ia_embedding\n",
    "            + self.w_ia_gradient * ia_gradient\n",
    "        )\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x2 = F.relu(self.fc2(x1))\n",
    "        x3 = self.fc3(x2)\n",
    "        return torch.sigmoid(x3)\n"
   ],
   "id": "67b4e544a0644a9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(\n",
    "        query: str, image_path: str, answer: str, model, processor, device\n",
    "):\n",
    "    #region Get inputs\n",
    "    # image + answer\n",
    "    i_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": answer}],\n",
    "        }\n",
    "    ]\n",
    "    i_inputs = processor.apply_chat_template(\n",
    "        i_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    i_inputs = {k: v.to(device) for k, v in i_inputs.items()}\n",
    "\n",
    "    # query + answer\n",
    "    q_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": query},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": answer}],\n",
    "        }\n",
    "    ]\n",
    "    q_inputs = processor.apply_chat_template(\n",
    "        q_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    q_inputs = {k: v.to(device) for k, v in q_inputs.items()}\n",
    "\n",
    "\n",
    "    # answer\n",
    "    a_messages = [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": answer}],\n",
    "        }\n",
    "    ]\n",
    "    a_inputs = processor.apply_chat_template(\n",
    "        a_messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    a_inputs = {k: v.to(device) for k, v in a_inputs.items()}\n",
    "    # endregion\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        model.eval()\n",
    "\n",
    "        q_output = model(**q_inputs, output_hidden_states=True)\n",
    "        i_output = model(**i_inputs, output_hidden_states=True)\n",
    "        a_output = model(**a_inputs, output_hidden_states=True)\n",
    "\n",
    "        q_length = q_inputs[\"input_ids\"].shape[1]\n",
    "        i_length = i_inputs[\"input_ids\"].shape[1]\n",
    "        a_length = a_inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Extract answer probs (slice after context)\n",
    "        q_prob = q_output.logits[0, q_length - (a_length - 1):, :]\n",
    "        i_prob = i_output.logits[0, i_length - (a_length - 1):, :]\n",
    "        a_prob = a_output.logits[0, 1:, :]\n",
    "\n",
    "        # Extract last hidden states (embeddings)\n",
    "        q_vector = q_output.hidden_states[-1]\n",
    "        i_vector = i_output.hidden_states[-1]\n",
    "        a_vector = a_output.hidden_states[-1]\n",
    "\n",
    "        a_embedding = a_vector[0, 1:, :]\n",
    "\n",
    "        # Question+answer embedding & gradient\n",
    "        qa_kl_divergence = torch.sum(\n",
    "            a_prob.softmax(dim=-1) * (a_prob.softmax(dim=-1).log() - torch.log_softmax(q_prob, dim=-1))\n",
    "        )\n",
    "        qa_gradient = torch.autograd.grad(\n",
    "            outputs=qa_kl_divergence, inputs=a_vector, create_graph=False, retain_graph=True,\n",
    "        )[0][0, 1:, :]\n",
    "\n",
    "        q_embedding = q_vector[0, q_length - (a_length - 1):, :]\n",
    "        qa_embedding = q_embedding - a_embedding\n",
    "\n",
    "        # Image+answer embedding & gradient\n",
    "        ia_kl_divergence = torch.sum(\n",
    "            a_prob.softmax(dim=-1) * (a_prob.softmax(dim=-1).log() - torch.log_softmax(i_prob, dim=-1))\n",
    "        )\n",
    "        ia_gradient = torch.autograd.grad(\n",
    "            outputs=ia_kl_divergence, inputs=a_vector, create_graph=False, retain_graph=True,\n",
    "        )[0][0, 1:, :]\n",
    "\n",
    "        i_embedding = i_vector[0, i_length - (a_length - 1):, :]\n",
    "        ia_embedding = i_embedding - a_embedding\n",
    "    return (\n",
    "        qa_embedding.detach().float().to(\"cpu\"),\n",
    "        qa_gradient.detach().float().to(\"cpu\"),\n",
    "        ia_embedding.detach().float().to(\"cpu\"),\n",
    "        ia_gradient.detach().float().to(\"cpu\"))\n",
    "\n",
    "def batch_extract_features(data_list, model, processor, device):\n",
    "    qa_embeddings = []\n",
    "    qa_gradients = []\n",
    "    ia_embeddings = []\n",
    "    ia_gradients = []\n",
    "\n",
    "    for data in tqdm(data_list, desc=\"Extract features:\"):\n",
    "        qa_embedding, qa_gradient, ia_embedding, ia_gradient = extract_features(\n",
    "            query = data['query'],\n",
    "            image_path= data['image_path'],\n",
    "            answer = data['answer'],\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            device=device\n",
    "        )\n",
    "        qa_embeddings.append(qa_embedding)\n",
    "        qa_gradients.append(qa_gradient)\n",
    "        ia_embeddings.append(ia_embedding)\n",
    "        ia_gradients.append(ia_gradient)\n",
    "\n",
    "        # Clear CUDA cache periodically\n",
    "        if len(qa_embeddings) % 10 == 0:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    return qa_embeddings, qa_gradients, ia_embeddings, ia_gradients"
   ],
   "id": "3abdaa96c51680c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class HallucinationDataset(Dataset):\n",
    "    def __init__(self, qa_embeddings, qa_gradients, ia_embeddings, ia_gradients, labels):\n",
    "        self.qa_embeddings = qa_embeddings\n",
    "        self.qa_gradients = qa_gradients\n",
    "        self.ia_embeddings = ia_embeddings\n",
    "        self.ia_gradients = ia_gradients\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.qa_embeddings[idx], self.qa_gradients[idx], self.ia_embeddings[idx], self.ia_gradients[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def hallucination_collate_fn(batch):\n",
    "    qa_embeddings = []\n",
    "    qa_gradients = []\n",
    "    ia_embeddings = []\n",
    "    ia_gradients = []\n",
    "    labels = []\n",
    "\n",
    "    for sample in batch:\n",
    "        qa_embeddings.append(sample[0])\n",
    "        qa_gradients.append(sample[1])\n",
    "        ia_embeddings.append(sample[2])\n",
    "        ia_gradients.append(sample[3])\n",
    "        labels.append(sample[4])\n",
    "    return qa_embeddings, qa_gradients, ia_embeddings, ia_gradients, torch.tensor(labels)\n",
    "\n",
    "def load_egh_dataset(folder_path) -> list:\n",
    "    dataset_path = folder_path + '/dataset.json'\n",
    "    images_path = folder_path + '/images/'\n",
    "    dataset = []\n",
    "\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data_list = json.load(f)\n",
    "    for data in data_list:\n",
    "        dataset.append({\n",
    "            \"id\": data['id'],\n",
    "            \"image_path\": images_path + data['image_id'],\n",
    "            \"query\": data['query'],\n",
    "            \"answer\": data['answer'],\n",
    "            \"label\": data['label']\n",
    "        })\n",
    "    print(f\"Successfully load the EHG dataset with: {len(dataset)} samples.\")\n",
    "    return dataset"
   ],
   "id": "3fee4df5d23d31e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(detector, loss_function, optimizer, data_loader):\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        qa_embedding, qa_gradient, ia_embedding, ia_gradient, label = batch\n",
    "        label = label.float()\n",
    "        output = detector(qa_embedding, qa_gradient, ia_embedding, ia_gradient).squeeze()\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    return total_loss\n",
    "\n",
    "def eval_detector(detector, data_loader):\n",
    "    total_label, total_pred, total_out = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(data_loader):\n",
    "            qa_embedding, qa_gradient, ia_embedding, ia_gradient, label = batch\n",
    "\n",
    "            output = detector(qa_embedding, qa_gradient, ia_embedding, ia_gradient).squeeze()\n",
    "            total_out += output.tolist()\n",
    "            total_label += label.tolist()\n",
    "            pred = list(map(lambda x: round(x), output.tolist()))\n",
    "            total_pred += pred\n",
    "        f1 = f1_score(total_label, total_pred)\n",
    "        acc = accuracy_score(total_label, total_pred)\n",
    "        precision, recall, cm = precision_recall_curve(total_label, total_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "    return acc, f1, pr_auc\n",
    "\n",
    "def load_dataset(dir_path, model, processor, device):\n",
    "    data_list = load_egh_dataset(dir_path)\n",
    "    qa_embedding, qa_gradient, ia_embedding, ia_gradient = (\n",
    "        batch_extract_features(data_list, model, processor, device))\n",
    "    labels = [data['label'] for data in data_list]\n",
    "    hidden_size = qa_embedding[0].size(-1)\n",
    "    return HallucinationDataset(\n",
    "        qa_embedding,\n",
    "        qa_gradient,\n",
    "        ia_embedding,\n",
    "        ia_gradient,\n",
    "        labels,\n",
    "    ), hidden_size"
   ],
   "id": "49a8769ceeee9e83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training",
   "id": "8384363e755a4444"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset_dir_path\", type=str, default=\"data/egh_vlm\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen3-VL-2B-Instruct\")\n",
    "parser.add_argument(\"--train_ratio\", type=float, default=0.5)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# VLM model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    args.model_name,\n",
    "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(args.model_name)"
   ],
   "id": "d713074a30ee6d06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset\n",
    "dataset, hidden_size = load_dataset(args.dataset_dir_path, model, processor, device)\n",
    "train_dataset = dataset\n",
    "test_dataset = dataset\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=hallucination_collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=hallucination_collate_fn,\n",
    "    shuffle=True,\n",
    ")"
   ],
   "id": "a1b9234894b80f63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training\n",
    "detector = DetectorModule(hidden_size, hidden_size, 1)\n",
    "epoch = 2\n",
    "loss_function = nn.BCELoss()\n",
    "optim = torch.optim.Adam(detector.parameters(), lr=1e-4)\n",
    "best_acc = 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "for i in range(epoch):\n",
    "    total_loss = train(loss_function, optim, train_dataloader)\n",
    "    print(f'Epoch [{i + 1}/{epoch}], Loss: {total_loss / 2000:.4f}')\n",
    "    acc, f1, pr_auc = eval_detector(detector, test_dataloader)\n",
    "    print(f'Epoch [{i + 1}/{epoch}], ACC: {acc:.4f}, F1: {f1:.4f}, PR-AUC:{pr_auc:.4f}')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        print(f'Best ACC at epoch {epoch + 1}')\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        print(f'Best F1 at epoch {epoch + 1}')\n",
    "\n",
    "    if total_loss < 1e-5:\n",
    "        break\n",
    "\n",
    "print(f'Training Finished!')"
   ],
   "id": "1f6d130290b13541"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
