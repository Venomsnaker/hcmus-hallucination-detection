{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241636e2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1439e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6f327",
   "metadata": {},
   "source": [
    "## Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efecc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verdict(response):\n",
    "    if \"yes\" in response.strip()[:10].lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def save_dataset(dataset, file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "def get_img_path(img_folder_path: str, img_name, dataset=\"phd\") -> str:\n",
    "    \"\"\"\n",
    "    dataset: \"phd\" or \"hallusion_bench\"\n",
    "    \"\"\"\n",
    "    if dataset == \"phd\":\n",
    "        for subfolder_name in [\"train2014\", \"val2014\"]:\n",
    "            subfolder_path = os.path.join(img_folder_path, subfolder_name)\n",
    "            if os.path.exists(subfolder_path):\n",
    "                local_img_name = f\"COCO_{subfolder_name}_{img_name}.jpg\"\n",
    "                img_path = os.path.join(subfolder_path, local_img_name)\n",
    "                if os.path.exists(img_path):\n",
    "                    return img_path \n",
    "        print(f\"Image {img_name} not found in PHD dataset.\")\n",
    "        return \"\"\n",
    "    elif dataset == \"hallusion_bench\":\n",
    "        return img_folder_path + img_name[1:]\n",
    "    else:\n",
    "        print(\"Dataset not recognized.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1764550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(messages, model, processor, max_new_tokens=64):\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return output_text[0] if len(output_text) == 1 else output_text\n",
    "\n",
    "def generate_answers(save_path, folder_path, file_name, img_dir_name=\"images\", sample_size=None, save_interval=20, ids_range=None):\n",
    "    # Load dataset\n",
    "    dataset_path = os.path.join(folder_path, file_name)\n",
    "    img_folder_path = os.path.join(folder_path, img_dir_name)\n",
    "    res = []\n",
    "    processed_ids = []\n",
    "\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "    if sample_size is not None and len(dataset) > sample_size:\n",
    "        dataset = dataset[:sample_size]\n",
    "\n",
    "    if ids_range is not None:\n",
    "        dataset = [d for d in dataset if d[\"id\"] >= ids_range[0] and d[\"id\"] <= ids_range[1]]\n",
    "    print(\"Dataset size after filtering by ids_range:\", len(dataset))\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            res = json.load(f)\n",
    "    processed_ids = [data[\"id\"] for data in res]\n",
    "\n",
    "    # Init model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "        \"Qwen/Qwen3-VL-2B-Instruct\",\n",
    "        dtype=\"auto\",\n",
    "        device_map=device\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        \"Qwen/Qwen3-VL-2B-Instruct\",\n",
    "        max_pixels=1280 * 720    \n",
    "    )\n",
    "\n",
    "    # Get responses\n",
    "    batch = []\n",
    "\n",
    "    for data in tqdm(dataset, desc=f\"Processing {file_name}:\"):\n",
    "        if data[\"id\"] in processed_ids:\n",
    "            continue\n",
    "        \n",
    "        img_path = get_img_path(img_folder_path, data[\"image_id\"], dataset=\"phd\")\n",
    "        messages = [\n",
    "            { \n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": img_path},\n",
    "                    {\"type\": \"text\", \"text\": data[\"question\"] + \"\\nAnswer ONLY in this exact format, make sure you add the explanation: yes/no, explanation based on what you see in the image.\"}],\n",
    "            }\n",
    "        ]\n",
    "        response = get_response(messages, model, processor)\n",
    "        \n",
    "        data[\"qwen3_vl_2b_response\"] = response\n",
    "        hallucinated_label = 0 if get_verdict(response) == data[\"label\"] else 1\n",
    "        data[\"hallucinated_label\"] = hallucinated_label\n",
    "        batch.append(data)\n",
    "\n",
    "        if len(batch) > save_interval:\n",
    "            res.extend(batch)\n",
    "            save_dataset(res, save_path)\n",
    "            batch.clear()\n",
    "    if batch:\n",
    "        res.extend(batch)\n",
    "        save_dataset(res, save_path)\n",
    "    print(f\"Completed. Dataset size: {len(res)}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f017521",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../data/phd/phd_sampled_qwen3_vl_2b.json\"\n",
    "folder_path = \"../data/phd\"\n",
    "file_name = \"phd_sampled.json\"\n",
    "\n",
    "'''\n",
    "Working directory:\n",
    "- phd\n",
    "    + phd_sampled.json\n",
    "    - images\n",
    "        - train2014\n",
    "        - val2014\n",
    "'''\n",
    "\n",
    "res = generate_answers(\n",
    "    save_path,\n",
    "    folder_path,\n",
    "    file_name,\n",
    "    # sample_size=None,\n",
    "    save_interval=20,\n",
    "    # ids_range=[0, 2500],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmus-hallucination-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
