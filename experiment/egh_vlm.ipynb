{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9053b0f2e6592137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "from egh_vlm.hallucination_dataset import HallucinationDataset, load_features, split_stratified, hallucination_collate_fn\n",
    "from egh_vlm.hallucination_detector import DetectorModule\n",
    "from egh_vlm.training import train_detector, eval_detector\n",
    "from egh_vlm.utils import ModelBundle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade7d5c7f64acdc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fc188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset_path', type=str, default='../data/phd/prototype/phd_sample_qwen3_vl_2b_balanced.json')\n",
    "parser.add_argument('--img_folder_path', type=str, default='../data/phd/images')\n",
    "parser.add_argument('--features_file_path', type=str, default='../data/phd/prototype/features_image_only_new.pt')\n",
    "parser.add_argument('--processed_features_file_path', type=str, default='../data/phd/prototype/features_image_only.pt')\n",
    "parser.add_argument('--detector_file_path', type=str, default='../data/phd/prototype/detector_image_only.pt')\n",
    "parser.add_argument('--model_name', type=str, default='Qwen/Qwen3-VL-2B-Instruct')\n",
    "parser.add_argument('--train_ratio', type=float, default=0.7)\n",
    "args = parser.parse_args('')\n",
    "config = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feab05eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device}')\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    config['model_name'],\n",
    "    dtype='auto',\n",
    "    device_map=device\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    config['model_name'],\n",
    "    max_pixels=1024 * 768)\n",
    "\n",
    "model_bundle = ModelBundle(model, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad206d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_raw_dataset(dataset):\n",
    "    label_counts = Counter(item['label'] for item in dataset)\n",
    "    print('Label distribution before balancing:\\n', label_counts)\n",
    "\n",
    "    min_count = min(label_counts.values())\n",
    "    print(f'Max balanced count per label: {min_count}')\n",
    "\n",
    "    balanced_dataset = []\n",
    "\n",
    "    for label in label_counts:\n",
    "        label_samples = [item for item in dataset if item['label'] == label]\n",
    "        balanced_dataset.extend(random.sample(label_samples, min_count))\n",
    "    print(f'Balanced dataset size: {len(balanced_dataset)}')\n",
    "    print('Label distribution after balancing:\\n', Counter(item['label'] for item in balanced_dataset))\n",
    "\n",
    "def get_balanced_hallucination_dataset(dataset):\n",
    "    indices_by_label = {label: [i for i, l in enumerate(dataset.labels) if l == label] \n",
    "                        for label in label_counts}\n",
    "    label_counts = Counter(dataset.labels)\n",
    "    min_count = min(label_counts.values())\n",
    "    balanced_indices = []\n",
    "\n",
    "    for label, indices in indices_by_label.items():\n",
    "        sample_size = min(min_count, len(indices))\n",
    "        balanced_indices.extend(random.sample(indices, sample_size))\n",
    "\n",
    "    balanced_ids = [dataset.ids[i] for i in balanced_indices]\n",
    "    balanced_embs = [dataset.embs[i] for i in balanced_indices]\n",
    "    balanced_grads = [dataset.grads[i] for i in balanced_indices]\n",
    "    balanced_labels = [dataset.labels[i] for i in balanced_indices]\n",
    "\n",
    "    print(f'Balanced dataset: {len(balanced_indices)} samples ({min_count} per {len(label_counts)} classes)')\n",
    "    return HallucinationDataset(balanced_ids, balanced_embs, balanced_grads, balanced_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7699d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of processed features: 3774\n",
      "Length of dataset: 3774\n",
      "Hidden size: 2048\n"
     ]
    }
   ],
   "source": [
    "def get_features(processed_path, save_path, dataset_path, img_folder_path, model_bundle: ModelBundle, mask_mode=None, sample_size=None):\n",
    "    if os.path.isfile(processed_path):\n",
    "        processed_features = load_features(processed_path)\n",
    "    else:\n",
    "        processed_features = None\n",
    "    print(f'Length of processed features: {len(processed_features.ids) if processed_features else 0}')\n",
    "\n",
    "    # dataset = load_phd_dataset(dataset_path, img_folder_path, sample_size=sample_size)\n",
    "    # features = batch_extract_features(dataset, model_bundle, processed_features, mask_mode, save_path)\n",
    "    features = processed_features\n",
    "    return features, features.embs[0].size(-1) if len(features) > 0 else 0\n",
    "\n",
    "dataset, hidden_size = get_features(\n",
    "    processed_path=args.processed_features_file_path,\n",
    "    save_path=args.features_file_path,\n",
    "    dataset_path=args.dataset_path,\n",
    "    img_folder_path=args.img_folder_path,\n",
    "    model_bundle=model_bundle,\n",
    "    mask_mode=None,\n",
    ")\n",
    "\n",
    "print('Length of dataset:', len(dataset))\n",
    "print('Hidden size:', hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b6bd6205aabb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T16:46:37.506770300Z",
     "start_time": "2026-01-11T16:46:37.494497Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_stratified(dataset, train_ratio=args.train_ratio, random_state=42)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=hallucination_collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=hallucination_collate_fn,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33ce97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline(weight, epoch, lr, save_path='log.txt'):\n",
    "    logging.basicConfig(filename=save_path, level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    result = {\n",
    "        'w': weight,\n",
    "        'train_ratio': args.train_ratio,\n",
    "        'epoch': epoch,\n",
    "        'lr': lr,\n",
    "        'acc': {\n",
    "            'value': 0.0,\n",
    "            'epoch': -1\n",
    "        },\n",
    "        'f1': {\n",
    "            'value': 0.0,\n",
    "            'epoch': -1\n",
    "        },\n",
    "        'pr_auc': {\n",
    "            'value': 0.0,\n",
    "            'epoch': -1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Init\n",
    "    detector = DetectorModule(hidden_size, hidden_size, 1, weight)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optim = torch.optim.Adam(detector.parameters(), lr=lr)\n",
    "    \n",
    "    logging.debug(f'Training w/ weight: {weight}')\n",
    "    for i in range(epoch):\n",
    "        total_loss = train_detector(detector, loss_fn, optim, train_dataloader)\n",
    "        acc, f1, pr_auc = eval_detector(detector, val_dataloader)\n",
    "        \n",
    "        logging.debug(f'Epoch [{i+1}/{epoch}], Loss: {total_loss:.4f}')\n",
    "        logging.debug(f'Epoch [{i + 1}/{epoch}], ACC: {acc:.4f}, F1: {f1:.4f}, PR-AUC:{pr_auc:.4f}\\n')\n",
    "        if acc > result['acc']['value']:\n",
    "            result['acc']['value'] = acc\n",
    "            result['acc']['epoch'] = i + 1\n",
    "        if f1 > result['f1']['value']:\n",
    "            result['f1']['value'] = f1\n",
    "            result['f1']['epoch'] = i + 1\n",
    "        if pr_auc > result['pr_auc']['value']:\n",
    "            result['pr_auc']['value'] = pr_auc\n",
    "            result['pr_auc']['epoch'] = i + 1\n",
    "        if total_loss < 1e-4:\n",
    "            break\n",
    "    logging.debug(f'Eval ACC: {result[\"acc\"][\"value\"]:.4f} at epoch {result[\"acc\"][\"epoch\"]}')\n",
    "    logging.debug(f'Eval F1: {result[\"f1\"][\"value\"]:.4f} at epoch {result[\"f1\"][\"epoch\"]}')\n",
    "    logging.debug(f'Eval PR-AUC: {result[\"pr_auc\"][\"value\"]:.4f} at epoch {result[\"pr_auc\"][\"epoch\"]}')\n",
    "\n",
    "    # Clean up logger handlers\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a2dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [round(0.1 * i, 2) for i in range(1, 10)]\n",
    "weights_test = [0.3, 0.5, 0.7]\n",
    "epoch = 30\n",
    "lr = 1e-4\n",
    "\n",
    "records = []\n",
    "\n",
    "for weight in weights:\n",
    "    result = training_pipeline(weight, epoch, lr, save_path=f'../data/logs/egh_vlm_image_only_{weight}_log.txt')\n",
    "    records.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8906009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/phd/egh_vlm_image_only_eval.json', 'w') as f:\n",
    "    json.dump(records, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmus-hallucination-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
