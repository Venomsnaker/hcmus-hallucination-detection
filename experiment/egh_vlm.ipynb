{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053b0f2e6592137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "from egh_vlm.extract_feature import batch_extract_features\n",
    "from egh_vlm.hallucination_dataset import HallucinationDataset, load_features, split_stratified, hallucination_collate_fn\n",
    "from egh_vlm.hallucination_detector import DetectorModule\n",
    "from egh_vlm.training import train_detector, eval_detector\n",
    "from egh_vlm.utils import ModelBundle, load_egh_dataset, load_phd_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963268b091c78b50",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    'Qwen/Qwen3-VL-2B-Instruct',\n",
    "    dtype='auto',\n",
    "    device_map=device\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    'Qwen/Qwen3-VL-2B-Instruct',\n",
    "    max_pixels=1280 * 720)\n",
    "model_bundle = ModelBundle(model, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f477642858bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_egh_dataset(folder_path = '../data/egh_vlm', file_name='egh_vlm.json', sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b8b8816693898",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = batch_extract_features(\n",
    "    dataset, model_bundle, mask_mode=None\n",
    ")\n",
    "if len(res) > 0:\n",
    "    print('Shape of embedding:', res.embs[0].shape)\n",
    "    print('Shape of gradient:', res.grads[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade7d5c7f64acdc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset_path', type=str, default='../data/phd/phd_sampled_qwen3_vl_2b_balanced.json')\n",
    "parser.add_argument('--img_folder_path', type=str, default='../data/phd/images')\n",
    "parser.add_argument('--features_file_path', type=str, default='../data/phd/features_question_only.pt')\n",
    "parser.add_argument('--detector_file_path', type=str, default='../data/phd/detector_question_only.pt')\n",
    "parser.add_argument('--model_name', type=str, default='Qwen/Qwen3-VL-2B-Instruct')\n",
    "parser.add_argument('--train_ratio', type=float, default=0.7)\n",
    "args = parser.parse_args('')\n",
    "config = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab05eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device used: {device}')\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    config['model_name'],\n",
    "    dtype='auto',\n",
    "    device_map=device\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    config['model_name'],\n",
    "    max_pixels=1024 * 768)\n",
    "\n",
    "model_bundle = ModelBundle(model, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad206d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_raw_dataset(dataset):\n",
    "    label_counts = Counter(item['label'] for item in dataset)\n",
    "    print('Label distribution before balancing:\\n', label_counts)\n",
    "\n",
    "    min_count = min(label_counts.values())\n",
    "    print(f'Max balanced count per label: {min_count}')\n",
    "\n",
    "    balanced_dataset = []\n",
    "\n",
    "    for label in label_counts:\n",
    "        label_samples = [item for item in dataset if item['label'] == label]\n",
    "        balanced_dataset.extend(random.sample(label_samples, min_count))\n",
    "    print(f'Balanced dataset size: {len(balanced_dataset)}')\n",
    "    print('Label distribution after balancing:\\n', Counter(item['label'] for item in balanced_dataset))\n",
    "\n",
    "def get_balanced_hallucination_dataset():\n",
    "    indices_by_label = {label: [i for i, l in enumerate(dataset.labels) if l == label] \n",
    "                        for label in label_counts}\n",
    "    label_counts = Counter(dataset.labels)\n",
    "    min_count = min(label_counts.values())\n",
    "    balanced_indices = []\n",
    "\n",
    "    for label, indices in indices_by_label.items():\n",
    "        sample_size = min(min_count, len(indices))\n",
    "        balanced_indices.extend(random.sample(indices, sample_size))\n",
    "\n",
    "    balanced_ids = [dataset.ids[i] for i in balanced_indices]\n",
    "    balanced_embs = [dataset.embs[i] for i in balanced_indices]\n",
    "    balanced_grads = [dataset.grads[i] for i in balanced_indices]\n",
    "    balanced_labels = [dataset.labels[i] for i in balanced_indices]\n",
    "\n",
    "    print(f'Balanced dataset: {len(balanced_indices)} samples ({min_count} per {len(label_counts)} classes)')\n",
    "    return HallucinationDataset(balanced_ids, balanced_embs, balanced_grads, balanced_labels)\n",
    "\n",
    "def get_features(processed_path, save_path, dataset_path, img_folder_path, model_bundle: ModelBundle, mask_mode=None, sample_size=None):\n",
    "    dataset = load_phd_dataset(dataset_path, img_folder_path, sample_size=sample_size)\n",
    "\n",
    "    if os.path.isfile(processed_path):\n",
    "        processed_features = load_features(processed_path)\n",
    "    else:\n",
    "        processed_features = None\n",
    "\n",
    "    print(f'Length: {len(processed_features.ids)}')\n",
    "    features = batch_extract_features(dataset, model_bundle, processed_features, mask_mode, save_path)\n",
    "    return features, features.embs[0].size(-1) if len(features) > 0 else 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7699d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully load the PhD dataset with: 3770 samples.\n",
      "Length: 3770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract features::  11%|â–ˆ         | 399/3770 [01:13<07:06,  7.91it/s]"
     ]
    }
   ],
   "source": [
    "dataset, hidden_size = get_features(\n",
    "    processed_path=args.features_file_path,\n",
    "    save_path='../data/phd/features_question_only_test.pt',\n",
    "    dataset_path=args.dataset_path,\n",
    "    img_folder_path=args.img_folder_path,\n",
    "    model_bundle=model_bundle,\n",
    "    mask_mode='image',\n",
    ")\n",
    "\n",
    "print('Length of dataset:', len(dataset))\n",
    "print('Hidden size:', hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6bd6205aabb0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T16:46:37.506770300Z",
     "start_time": "2026-01-11T16:46:37.494497Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_stratified(dataset, train_ratio=args.train_ratio, random_state=42)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=hallucination_collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=hallucination_collate_fn,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4640f5955ad4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T16:46:47.940655300Z",
     "start_time": "2026-01-11T16:46:47.782611900Z"
    }
   },
   "outputs": [],
   "source": [
    "detector = DetectorModule(hidden_size, hidden_size, 1, 0.2)\n",
    "epoch = 20\n",
    "loss_function = nn.BCELoss()\n",
    "optim = torch.optim.Adam(detector.parameters(), lr=1e-4)\n",
    "best_acc = [0.0, -1]\n",
    "best_f1 = [0.0, -1]\n",
    "best_pr_auc = [0.0, -1]\n",
    "\n",
    "for i in range(epoch):\n",
    "    total_loss = train_detector(detector, loss_function, optim, train_dataloader)\n",
    "    print(f'Epoch [{i + 1}/{epoch}], Loss: {total_loss / 2000:.4f}')\n",
    "    acc, f1, pr_auc = eval_detector(detector, val_dataloader)\n",
    "    print(f'Epoch [{i + 1}/{epoch}], ACC: {acc:.4f}, F1: {f1:.4f}, PR-AUC:{pr_auc:.4f}')\n",
    "\n",
    "    if acc > best_acc[0]:\n",
    "        best_acc = [acc, i + 1]\n",
    "    if f1 > best_f1[0]:\n",
    "        best_f1 = [f1, i + 1]\n",
    "    if pr_auc > best_pr_auc[0]:\n",
    "        best_pr_auc = [pr_auc, i + 1]\n",
    "    if total_loss < 1e-3:\n",
    "        break\n",
    "\n",
    "print(f'Eval ACC: {best_acc[0]:.4f} at epoch {best_acc[1]}')\n",
    "print(f'Eval F1: {best_f1[0]:.4f} at epoch {best_f1[1]}')\n",
    "print(f'Eval PR-AUC: {best_pr_auc[0]:.4f} at epoch {best_pr_auc[1]}')\n",
    "\n",
    "torch.save(detector.state_dict(), args.detector_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmus-hallucination-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
